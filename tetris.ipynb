{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn0MZ0mGAn3hXZdZ0J+lbc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epeay/tetris-ml/blob/main/tetris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VaWEj4SZvRKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba03e41-16bf-4691-ba24-e867121427e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "\n",
        "\n",
        "class TetrominoPiece:\n",
        "\n",
        "    BLOCK = '▆'\n",
        "\n",
        "    def __init__(self, shape, patterns):\n",
        "        self.shape = shape\n",
        "        self.patterns = patterns\n",
        "        self.rot = 0\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"TetrominoPiece(shape={Tetrominos.shape_name(self.shape)}, rot={self.rot*90}, pattern= {self.printable_pattern(oneline=True)})\"\n",
        "\n",
        "    def printable_pattern(self, oneline=False):\n",
        "        ret = []\n",
        "        pattern = self.get_pattern()\n",
        "        for i, row in enumerate(pattern):\n",
        "            row_str = \" \".join([str(c) for c in row])\n",
        "            ret.append(row_str)\n",
        "\n",
        "            if not oneline:\n",
        "                ret.append(\"\\n\")\n",
        "            else:\n",
        "                if i < len(pattern)-1:\n",
        "                    ret.append(\" / \",)\n",
        "        ret = \"\".join(ret).replace('1', TetrominoPiece.BLOCK).replace('0', '_')\n",
        "        return \"\".join(ret)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_pattern(self):\n",
        "        return self.patterns[self.rot]\n",
        "\n",
        "    def rotate(self):\n",
        "        \"\"\"Rotates IN PLACE, and returns the new pattern\"\"\"\n",
        "        self.rot = (self.rot + 1) % 4\n",
        "        return self.get_pattern()\n",
        "\n",
        "    def get_height(self):\n",
        "        return len(self.patterns[self.rot])\n",
        "\n",
        "    def get_width(self):\n",
        "        return max([len(x) for x in self.patterns[self.rot]])\n",
        "\n",
        "    def get_bottom_offsets(self):\n",
        "        \"\"\"\n",
        "        For each column in the shape, returns the gap between the bottom of\n",
        "        the shape (across all columns) and the bottom of the shape in that\n",
        "        column.\n",
        "\n",
        "        Returned values in the list would expect to contain at least one 0, and\n",
        "        no values higher than the height of the shape.\n",
        "\n",
        "        For example, an S piece:\n",
        "        _ X X\n",
        "        X X _\n",
        "\n",
        "        Would have offsets [0, 0, 1] in this current rotation. This method is\n",
        "        used in determining if a piece will fit at a certain position\n",
        "        in the board.\n",
        "        \"\"\"\n",
        "        # print(self)\n",
        "        pattern = self.get_pattern()\n",
        "        # pdb.set_trace()\n",
        "        ret = [len(pattern)+1 for x in range(len(pattern[0]))]\n",
        "        # Iterates rows from top, down\n",
        "        for ri in range(len(pattern)):\n",
        "            # Given a T shape:\n",
        "            # X X X\n",
        "            # _ X _\n",
        "            # Start with row [X X X] (ri=0, offset=1)\n",
        "            row = pattern[ri]\n",
        "            # print(f\"Testing row {row} at index {ri}\")\n",
        "            for ci, col in enumerate(row):\n",
        "                if col == 1:\n",
        "                    offset = len(pattern) - ri - 1\n",
        "                    ret[ci] = offset\n",
        "\n",
        "            # Will return [1, 0, 1] for a T shape\n",
        "\n",
        "        if max(ret) >= len(pattern):\n",
        "          print(f\"Pattern:\")\n",
        "          print(pattern)\n",
        "          print(f\"Bottom Offsets: {ret}\")\n",
        "          print(f\"Shape: {self.shape}\")\n",
        "          raise ValueError(\"Tetromino pattern has incomplete bottom offsets\")\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def get_top_offsets(self):\n",
        "        \"\"\"\n",
        "        Returns the height of the shape at each column.\n",
        "\n",
        "        For example, an S piece:\n",
        "        _ X X\n",
        "        X X _\n",
        "\n",
        "        Would have offsets [1, 2, 2] in this current rotation. This provides\n",
        "        guidance on how to update the headroom list.\n",
        "\n",
        "        Ideally we should cache this.\n",
        "        \"\"\"\n",
        "        pattern = self.get_pattern()\n",
        "        ret = [0 for x in len(pattern[0])]\n",
        "        for ri, row in enumerate(range(pattern, )):\n",
        "            for col in pattern[row]:\n",
        "                if pattern[row][col] == 1:\n",
        "                    ret[col] = max(ret[col], row)\n",
        "        return ret\n",
        "\n",
        "\n",
        "class Tetrominos:\n",
        "    O = 1\n",
        "    I = 2\n",
        "    S = 3\n",
        "    Z = 4\n",
        "    T = 5\n",
        "    J = 6\n",
        "    L = 7\n",
        "\n",
        "    base_patterns = {\n",
        "        # X X\n",
        "        # X X\n",
        "        O: np.array([[1, 1], [1, 1]]),\n",
        "\n",
        "        # X X X X\n",
        "        I: np.array([[1, 1, 1, 1]]),\n",
        "\n",
        "        # _ X X\n",
        "        # X X _\n",
        "        S: np.array([[0, 1, 1], [1, 1, 0]]),\n",
        "        Z: np.array([[1, 1, 0], [0, 1, 1]]),\n",
        "        T: np.array([[1, 1, 1], [0, 1, 0]]),\n",
        "        J: np.array([[1, 0, 0], [1, 1, 1]]),\n",
        "        L: np.array([[0, 0, 1], [1, 1, 1]])\n",
        "    }\n",
        "\n",
        "    # Stores patterns for each tetromino, at each rotation\n",
        "    cache = {}\n",
        "\n",
        "    def num_tetrominos():\n",
        "        return len(Tetrominos.base_patterns.keys())\n",
        "\n",
        "    @staticmethod\n",
        "    def shape_name(shape):\n",
        "        if shape == Tetrominos.O:\n",
        "            return \"O\"\n",
        "        elif shape == Tetrominos.I:\n",
        "            return \"I\"\n",
        "        elif shape == Tetrominos.S:\n",
        "            return \"S\"\n",
        "        elif shape == Tetrominos.Z:\n",
        "            return \"Z\"\n",
        "        elif shape == Tetrominos.T:\n",
        "            return \"T\"\n",
        "        elif shape == Tetrominos.J:\n",
        "            return \"J\"\n",
        "        elif shape == Tetrominos.L:\n",
        "            return \"L\"\n",
        "        else:\n",
        "            raise ValueError(\"Invalid shape\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def make(shape):\n",
        "        \"\"\"\n",
        "        shape:\n",
        "        \"\"\"\n",
        "        if not Tetrominos.cache:\n",
        "            for shape, pattern in Tetrominos.base_patterns.items():\n",
        "                Tetrominos.cache[shape] = [\n",
        "                    pattern,\n",
        "                    np.rot90(pattern),\n",
        "                    np.rot90(pattern, 2),\n",
        "                    np.rot90(pattern, 3)\n",
        "                ]\n",
        "\n",
        "\n",
        "        if shape not in Tetrominos.base_patterns.keys():\n",
        "            raise ValueError(\"Invalid shape\")\n",
        "\n",
        "        return TetrominoPiece(shape, Tetrominos.cache[shape])\n",
        "\n",
        "class TetrisBoard:\n",
        "\n",
        "    BLOCK = '▆'\n",
        "\n",
        "    def __init__(self, height, width):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.height, self.width), dtype=int)\n",
        "        self.headroom = [self.height for _ in range(self.width)]\n",
        "        self.piece = None\n",
        "\n",
        "    def remove_tetris(self):\n",
        "        to_delete = []\n",
        "        for r, row in enumerate(self.board):\n",
        "            if sum(row) == self.width:\n",
        "                to_delete.append(r)\n",
        "\n",
        "        if to_delete:\n",
        "          self.board = np.delete(self.board, to_delete, axis=0)\n",
        "          self.board.resize((self.height, self.width))\n",
        "          # pdb.set_trace()\n",
        "\n",
        "    def place_piece(self, piece:TetrominoPiece, logical_coords):\n",
        "        \"\"\"\n",
        "        Places a piece at the specified column. Dynamically calculates correct\n",
        "        height for the piece.\n",
        "\n",
        "        piece: a TetrominoPiece object\n",
        "        logical_coords: The logical row and column for the bottom left\n",
        "            of the piece's pattern\n",
        "        \"\"\"\n",
        "        pattern = piece.get_pattern()\n",
        "        bottom_offsets = np.array(piece.get_bottom_offsets())\n",
        "        # TODO don't calculate all bottoms because we don't need them all\n",
        "\n",
        "        lrow = logical_coords[0]\n",
        "        lcol = logical_coords[1]\n",
        "\n",
        "        p_height = piece.get_height()\n",
        "\n",
        "        for r in range(p_height):\n",
        "            pattern_row = pattern[len(pattern)-1-r]\n",
        "            board_row = self.board[lrow-1+r]\n",
        "\n",
        "            for i, c in enumerate(pattern_row):\n",
        "                # Iff c is 1, push it to the board\n",
        "                board_row[lcol-1+i] |= c\n",
        "\n",
        "\n",
        "    def find_logical_BL_placement(self, piece:TetrominoPiece, col):\n",
        "        \"\"\"\n",
        "        Returns the logical row and column of the bottom left corner of the\n",
        "        pattern, such that when placed, the piece will sit flush against existing\n",
        "        tower parts, and not exceed the max board height.\n",
        "\n",
        "        Given:\n",
        "        BOARD       PIECE\n",
        "        5 _ _ _ _\n",
        "        4 _ _ _ X\n",
        "        3 _ _ X X   X X X X\n",
        "        2 _ X X _\n",
        "        1 X X X X\n",
        "\n",
        "        Returns (5, 1)\n",
        "\n",
        "        Given:\n",
        "        BOARD       PIECE    COL\n",
        "        5 _ _ _ _\n",
        "        4 _ _ _ X\n",
        "        3 _ _ X X   X X X    1 (lcol 2)\n",
        "        2 _ X X _     X\n",
        "        1 X X X X\n",
        "\n",
        "        Returns (3, 1)\n",
        "\n",
        "        piece: a TetrominoPiece object\n",
        "        col: zero-index column to place the 0th column of the piece.\n",
        "        \"\"\"\n",
        "        pattern = piece.get_pattern()\n",
        "        bottom_offsets = np.array(piece.get_bottom_offsets())\n",
        "        # TODO don't calculate all bottoms because we don't need them all\n",
        "        board_heights = np.array(self.get_tops()[col:col+piece.get_width()])\n",
        "\n",
        "        # Given:\n",
        "        # BOARD       PIECE\n",
        "        # 5 _ _ _ _\n",
        "        # 4 _ _ _ X\n",
        "        # 3 _ _ X X   X X X X\n",
        "        # 2 _ X X _\n",
        "        # 1 X X X X\n",
        "        # Tops -> [1,2,3,4]\n",
        "        #\n",
        "        # The sideways I has bottom offsets [0,0,0,0]\n",
        "        # Start at min(board_tops)+1 and try to place the piece.\n",
        "        #\n",
        "        # If placing on row 2, the piece heights would be [2,2,2,2]\n",
        "        # Board heights are [1,2,3,4], so this\n",
        "        # doesn't clear the board for all columns. Try placing on row 3.\n",
        "        # [3,3,3,3] > [1,2,3,4] ? False\n",
        "        # Try row 4... False. Try row 5...\n",
        "        # [5,5,5,5] > [1,2,3,4] ? True\n",
        "        # So we place the piece on row 5 (index 4)\n",
        "        #\n",
        "        # 5 X X X X\n",
        "        # 4 _ _ _ X\n",
        "        # 3 _ _ X X\n",
        "        # 2 _ X X _\n",
        "        # 1 X X X X\n",
        "        # (yes, this is a horrible move)\n",
        "\n",
        "        p_height = piece.get_height()\n",
        "        p_width = piece.get_width()\n",
        "        can_place = False\n",
        "\n",
        "        # TODO Pick better min test height\n",
        "        # If there's a very narrow, tall tower, and you're placing a flat I\n",
        "        # just to the left of it, you'll likely test placement for each level of\n",
        "        # the tower until the piece clears it.\n",
        "        for place_row in range(min(board_heights)+1, max(board_heights)+2):\n",
        "            # In the example, place_row would be 2...3...4...5\n",
        "\n",
        "            # Is [2,2,2,2] > [1,2,3,4] ?\n",
        "            # Does this placement not interfere with existing board pieces?\n",
        "            # print(f\"Trying placement at row {place_row}\")\n",
        "            # print(f\"{(bottom_offsets + place_row)} > {board_heights}\")\n",
        "\n",
        "\n",
        "\n",
        "            bottom_clears_board = all((bottom_offsets + place_row) > board_heights)\n",
        "\n",
        "            if not bottom_clears_board:\n",
        "                continue\n",
        "\n",
        "            # Check the final height\n",
        "            if place_row-1 + p_height > self.height:\n",
        "                raise ValueError(f\"Requested placement at col {col+1} would require rows {place_row}-{place_row-1 + p_height}. Piece {piece}\")\n",
        "\n",
        "            can_place = True\n",
        "            break\n",
        "\n",
        "        if not can_place:\n",
        "            # pdb.set_trace()\n",
        "            raise ValueError(f\"Piece failed to be placed at lcolumn {col+1}\")\n",
        "\n",
        "        return (place_row, col+1)\n",
        "\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        output = False\n",
        "        for i, row in enumerate(reversed(self.board)):\n",
        "            if sum(row) == 0 and not output:\n",
        "                continue\n",
        "            else:\n",
        "                output = True\n",
        "\n",
        "            for cell in row:\n",
        "                if cell == 1:\n",
        "                    print(TetrisBoard.BLOCK, end=' ')\n",
        "                else:\n",
        "                    print('_', end=' ')\n",
        "            print()\n",
        "\n",
        "        if not output:\n",
        "            print(\"<<EMPTY BOARD>>\")\n",
        "\n",
        "\n",
        "\n",
        "    def get_tops(self):\n",
        "        \"\"\"\n",
        "        Gets the height of each column on the board.\n",
        "        This is gonna be inefficient for now.\n",
        "\n",
        "        A board with only an I at the left side would return [4, 0, 0, ...]\n",
        "        \"\"\"\n",
        "        tops = [0 for _ in range(self.width)]\n",
        "        for r, row in enumerate(self.board):\n",
        "            if sum(row) == 0:\n",
        "                break\n",
        "\n",
        "            for col, val in enumerate(row):\n",
        "                if val == 1:\n",
        "                    tops[col] = r+1\n",
        "\n",
        "        return tops\n",
        "\n",
        "\n",
        "\n",
        "class TetrisGameStats:\n",
        "    def __init__(self):\n",
        "        self.moves = 0\n",
        "        self.lines_cleared = 0\n",
        "        self.cleared_by_size = {\n",
        "            1: 0,\n",
        "            2: 0,\n",
        "            3: 0,\n",
        "            4: 0\n",
        "        }\n",
        "\n",
        "class TetrisEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(TetrisEnv, self).__init__()\n",
        "        self.board_height = 20\n",
        "        self.board_width = 10\n",
        "        self.board = TetrisBoard(self.board_height, self.board_width)\n",
        "        self.current_piece = None\n",
        "        self.pieces = Tetrominos()\n",
        "        self.reward_history = deque(maxlen=10)\n",
        "\n",
        "        # Action space: tuple (column, rotation)\n",
        "        # TODO Limit action width properly\n",
        "        self.action_space = spaces.MultiDiscrete([self.board_width, 4])\n",
        "\n",
        "        # Observation space: the board state\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=1,\n",
        "            shape=(self.board_height * self.board_width + Tetrominos.num_tetrominos(),),\n",
        "            dtype=int\n",
        "            )\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board.reset()\n",
        "        self.current_piece = self._get_random_piece()\n",
        "        return self._get_board_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        col, rotation = action\n",
        "\n",
        "        # Rotate the piece to the desired rotation\n",
        "        for _ in range(rotation):\n",
        "            self.current_piece.rotate() # Rotates IN PLACE\n",
        "\n",
        "        if not self._is_valid_action(self.current_piece, col+1):\n",
        "            # We may resolve this with a \"redo\" instead of stopping the episode\n",
        "            # in the future.\n",
        "            # print(\"Invalid Action\")\n",
        "            # self.board.render()\n",
        "            # print(f\"Action Column: {col+1} (1-{self.board_width})\")\n",
        "            # print(f\"Piece: {self.current_piece}\")\n",
        "            done = False\n",
        "            # print(\">>> REDO\")\n",
        "            reward = self._calculate_reward() * 0.5\n",
        "            return self._get_board_state(), reward, done, {}\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Find where the piece would sit on the board\n",
        "            lcoords = self.board.find_logical_BL_placement(self.current_piece, col)\n",
        "        except ValueError as e:\n",
        "            # print(f\"Exception: {e}\")\n",
        "            done = True\n",
        "            # TODO Account for a fatal placement\n",
        "            # self.board.render()\n",
        "            # print(f\"Action Column: {col+1} (1-{self.board_width})\")\n",
        "            # print(f\"Piece: {self.current_piece}\")\n",
        "            reward = self._calculate_reward() * 0.5\n",
        "            return self._get_board_state(), reward, done, {}\n",
        "\n",
        "\n",
        "        self.board.place_piece(self.current_piece, lcoords)\n",
        "        reward = self._calculate_reward()\n",
        "        self.reward_history.append(reward)\n",
        "        done = self._is_done()\n",
        "        self.board.remove_tetris()\n",
        "        self.current_piece = self._get_random_piece()\n",
        "\n",
        "        next_state = self._get_board_state()\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        self.board.render()\n",
        "\n",
        "    def _get_random_piece(self):\n",
        "        return self.pieces.make(np.random.randint(1, 8))\n",
        "\n",
        "    def _is_valid_action(self, piece, lcol):\n",
        "        piece = self.current_piece\n",
        "\n",
        "        if lcol < 1 or lcol > self.board_width:\n",
        "            # print(\"col out of range\")\n",
        "            return False\n",
        "\n",
        "        # An O on col 1 would take up cols 1-2\n",
        "        if lcol + piece.get_width() -1 > self.board_width:\n",
        "            # print(\"col + width out of range\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "\n",
        "        # Evaluate line pack\n",
        "        # Packed lines produces a higher score\n",
        "        # Big narrow tower would produce a low score\n",
        "        active_lines = 0\n",
        "        board_tiles = 0\n",
        "        lines_cleared = 0\n",
        "        for row in self.board.board:\n",
        "            row_sum = sum(row)\n",
        "            board_tiles += row_sum\n",
        "            if row_sum == 0:\n",
        "                continue\n",
        "\n",
        "            active_lines += 1\n",
        "            if row_sum == self.board.width:\n",
        "                lines_cleared += 1\n",
        "\n",
        "        if active_lines == 0:\n",
        "            return 0\n",
        "\n",
        "        # Simulating an extra 5 packed tiles per line cleared\n",
        "        line_score = (board_tiles+(5*lines_cleared)) / float(self.board_width * active_lines)\n",
        "        reward = line_score  # That's all for now\n",
        "        return reward\n",
        "\n",
        "    def _is_done(self):\n",
        "        return False\n",
        "\n",
        "    def _get_board_state(self):\n",
        "        # Get the current board state\n",
        "        board_state = self.board.board.flatten()\n",
        "\n",
        "        # Create a one-hot encoding for the current piece\n",
        "        piece_one_hot = np.zeros(Tetrominos.num_tetrominos())\n",
        "        piece_one_hot[self.current_piece.shape - 1] = 1\n",
        "\n",
        "        # Concatenate the board state and the one-hot encoding\n",
        "        return np.concatenate([board_state, piece_one_hot])\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  # Example usage\n",
        "  env = TetrisEnv()\n",
        "  state = env.reset()\n",
        "\n",
        "  done = False\n",
        "  loop_limit = 10\n",
        "  loop = 0\n",
        "  while not done and loop < loop_limit:\n",
        "      action = env.action_space.sample()  # Random action for demonstration\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      env.board.render()\n",
        "      print(f\"Reward: {reward}, Done: {done}\")\n",
        "      loop += 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6yLLSvNjgz1a"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995, min_exploration_rate=0.01, replay_buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim[0] * action_dim[1]  # Total number of actions\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.min_exploration_rate = min_exploration_rate\n",
        "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = DQN(state_dim, self.action_dim)\n",
        "        self.target_model = DQN(state_dim, self.action_dim)\n",
        "        self.update_target_model()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def guess(self, state):\n",
        "        return (random.randint(0, self.action_dim // 4 - 1), random.randint(0, 4 - 1))\n",
        "\n",
        "    def predict(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = self.model(state)\n",
        "        action_index = torch.argmax(q_values).item()\n",
        "        return (action_index // 4, action_index % 4)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return self.guess(state)\n",
        "        else:\n",
        "            return self.predict(state)\n",
        "\n",
        "    def evaluate(self, env, num_episodes=10):\n",
        "        total_rewards = []\n",
        "        for _ in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.predict(state)  # Always use the learned policy\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "            total_rewards.append(total_reward)\n",
        "        return total_rewards\n",
        "\n",
        "\n",
        "    def train(self, env, num_episodes=10):\n",
        "        total_rewards = []\n",
        "        target_update_interval = 10\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = env.reset().flatten()  # Flatten the state to fit the input of the network\n",
        "            step_count = 0\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = agent.choose_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                step_count += 1\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                self.replay()\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            self.decay_exploration_rate()\n",
        "            total_rewards.append(total_reward)\n",
        "\n",
        "            if episode % target_update_interval == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "        return total_rewards\n",
        "\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # print(\">>>> REPLAY\")\n",
        "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor([a[0] * 4 + a[1] for a in actions])\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        # Calculate current Q values\n",
        "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Calculate next Q values using the target model\n",
        "        next_q_values = self.target_model(next_states).max(1)[0]\n",
        "        target_q_values = rewards + self.discount_factor * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "\n",
        "        # Perform the optimization step\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def decay_exploration_rate(self):\n",
        "        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)\n"
      ],
      "metadata": {
        "id": "-5CwQMy0yPYd"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Tetris environment\n",
        "env = TetrisEnv()\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.nvec\n",
        "\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "num_episodes = 10\n",
        "target_update_interval = 10\n",
        "\n",
        "training_tracker = []\n",
        "\n",
        "for i in range(10):\n",
        "    rewards = agent.train(env, 100)\n",
        "    training_tracker.append((\"TRAIN\", rewards))\n",
        "    print(rewards)\n",
        "    print(f\"Rewards Average: {np.average(rewards)}\")\n",
        "    print(f\"Rewards Max: {max(rewards)}\")\n",
        "    print(f\"Rewards Min: {min(rewards)}\")\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    rewards = agent.evaluate(env, 100)\n",
        "    print(rewards)\n",
        "    training_tracker.append((\"EVALUATE\", rewards))\n",
        "    print(f\"Rewards Average: {np.average(rewards)}\")\n",
        "    print(f\"Rewards Max: {max(rewards)}\")\n",
        "    print(f\"Rewards Min: {min(rewards)}\")\n",
        "    rewards = agent.train(env, 100)\n",
        "    print(rewards)\n",
        "    training_tracker.append((\"TRAIN\", rewards))\n",
        "    print(f\"Rewards Average: {np.average(rewards)}\")\n",
        "    print(f\"Rewards Max: {max(rewards)}\")\n",
        "    print(f\"Rewards Min: {min(rewards)}\")\n",
        "\n",
        "\n",
        "\n",
        "# for episode in range(num_episodes):\n",
        "#     state = env.reset().flatten()  # Flatten the state to fit the input of the network\n",
        "#     step_count = 0\n",
        "#     total_reward = 0\n",
        "#     done = False\n",
        "\n",
        "#     while not done:\n",
        "#         action = agent.choose_action(state)\n",
        "#         next_state, reward, done, _ = env.step(action)\n",
        "#         step_count += 1\n",
        "#         print(f\"Step Reward: {reward}\")\n",
        "#         next_state = next_state.flatten()\n",
        "\n",
        "#         agent.remember(state, action, reward, next_state, done)\n",
        "#         agent.replay()\n",
        "#         state = next_state\n",
        "#         total_reward += reward\n",
        "\n",
        "#     agent.decay_exploration_rate()\n",
        "\n",
        "#     if episode % target_update_interval == 0:\n",
        "#         agent.update_target_model()\n",
        "\n",
        "#     print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Exploration Rate: {agent.exploration_rate}, Replay Buffer Size:{len(agent.replay_buffer)}\")\n",
        "#     print(f\"Step Count: {step_count}\")\n",
        "#     print(\"----------------------------------------------------------------\")\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G-4Y3mgHvlrH",
        "outputId": "198b7076-6622-4470-db17-30f0cecd54b4"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.5502923976608183, 9.080976800976801, 10.688333333333334, 4.142711359847583, 7.886651772719885, 9.567612129872193, 8.22600887674417, 8.677860309625018, 14.39871293996681, 7.632653352839113, 12.273096679722066, 8.782380952380953, 7.953935015792603, 9.04274690222059, 11.31838383838384, 4.821406634301371, 9.425794924030221, 6.057850877192982, 7.086176470588237, 8.510944026733501, 9.747065989565995, 8.004693289166976, 13.491543631806788, 7.998974358974362, 7.500975299210595, 5.756897050318104, 7.462592466013518, 8.685555555555558, 6.925844155844158, 8.289243044537162, 7.110765550239237, 4.8654322079709065, 6.826222731516848, 8.895709728867626, 6.040420168067226, 7.917013121037891, 11.766673706410549, 11.64118835412953, 11.446538315485684, 10.465122655122656, 8.940743570155336, 11.307097721679765, 5.9435324479442135, 7.260100817654998, 11.15463118580766, 6.084945697577276, 10.855032679738565, 2.999908424908425, 8.533796922032217, 11.798793363499247, 6.438838383838384, 10.870542986425338, 9.792647058823531, 7.5196810207336515, 6.931148459383755, 6.012941176470588, 6.8629411764705885, 5.361262626262627, 8.665000000000001, 8.354105339105338, 10.078432269484903, 10.870406445979201, 7.6057614624797285, 6.945252525252525, 6.660193140193139, 13.299386789680906, 12.13407096171802, 6.718698830409356, 10.600972915972918, 8.585396825396826, 10.545755225166992, 8.343812812574425, 4.4629949874686705, 5.942378003833111, 7.886274509803922, 9.154143634143635, 11.259103820156456, 7.430862470862473, 5.984320189614307, 9.783068924539512, 7.3787071846282375, 8.684947927889102, 8.357777777777779, 9.606468868249054, 9.101439087228563, 3.5108447589562144, 7.879743589743591, 11.59912087912088, 6.216733349210129, 11.522966615427919, 10.474497518305569, 11.570058479532161, 8.590451127819549, 10.12136714334857, 10.348568196509373, 7.0419841269841275, 5.309053724053723, 8.070470085470086, 8.173529411764706, 15.411320346320341]\n",
            "Rewards Average: 8.549120289763557\n",
            "Rewards Max: 15.411320346320341\n",
            "Rewards Min: 2.999908424908425\n",
            "[6.664040404040405, 8.191379789800843, 9.30113886113886, 5.432979389032021, 9.80652882205514, 8.976696549064972, 10.811256243756246, 10.388862190441142, 7.674820081878905, 11.69259619637329, 10.917971021066998, 9.950777849601382, 9.875147143877795, 9.447367125478584, 6.437104072398189, 9.39310727799892, 8.630606060606064, 8.361493477282954, 8.620931003980541, 8.414112554112558, 11.5009940034166, 10.07136598867249, 9.798896346171887, 7.17409645909646, 10.116197574355473, 13.45932319142846, 8.593919022154315, 8.73217589428116, 9.480725853094274, 10.659163416562798, 8.907991452991457, 10.010810359231414, 8.66063492063492, 8.624409771515037, 6.458388618605335, 7.914018870706177, 12.40309555941135, 8.804580909286791, 10.426729644624379, 9.299441049146933, 12.65134956546721, 7.0857640725287805, 8.699761904761905, 4.9159884559884555, 12.361883116883117, 8.984869459745623, 6.524432234432235, 11.227613585245164, 15.03365961523856, 9.361705451411336, 12.989487179487185, 7.785960020371786, 11.083689578395465, 9.809771967798286, 6.073715728715729, 5.639695696460402, 13.648046398046397, 7.082301587301588, 12.542950937950934, 5.126673414304993, 9.084798225510301, 11.038286022542987, 8.658582428922989, 9.285541125541128, 15.25401650636945, 9.898394873100756, 11.768461538461542, 9.406117216117217, 11.657396655631953, 5.399928698752228, 13.490971479500887, 7.884817260482893, 14.116969696969706, 8.792443609022557, 11.896106442577036, 9.474665680037196, 11.575486539604187, 11.278786181139127, 10.303333333333335, 8.906756576756578, 10.037755804566958, 10.580660474716204, 9.208225108225108, 7.801336032388665, 12.061761310584835, 7.2518341307815, 10.13300296195033, 19.237798280151225, 15.864798371563078, 10.817943722943724, 17.29555424193969, 10.580683760683762, 8.62562355085575, 6.869399138872824, 5.99159410764674, 14.897123062695822, 8.449253687488982, 9.29850561203503, 13.728178780284045, 19.26987007488555]\n",
            "Rewards Average: 9.958879572955425\n",
            "Rewards Max: 19.26987007488555\n",
            "Rewards Min: 4.9159884559884555\n",
            "[15.728466174348526, 14.403867243867243, 4.5455452390746505, 9.018277115041823, 11.65458195399372, 19.08384920634921, 8.977857142857141, 15.602166311500673, 13.90780554326994, 10.539074334678048, 16.680728715728716, 13.841538461538464, 12.97554673396779, 15.005694852103517, 5.332283359914939, 12.050705194358448, 11.096194916194918, 12.267880832617674, 9.668404344193819, 10.7063308913309, 8.719826839826842, 11.223321293429656, 8.960851648351651, 6.57783757350321, 11.461904761904758, 11.544013710747459, 11.3459126984127, 15.542252388134736, 10.773872924461163, 10.155481560063608, 9.247527982853061, 17.33402962534542, 17.156923076923075, 8.100000000000001, 8.460720361509836, 6.763247863247863, 17.38755633255633, 12.14800865800866, 8.445116713352007, 11.066625727213973, 7.1221774304127266, 8.743706173427535, 12.572102627334823, 11.302333775615509, 10.89087301587302, 12.7584595959596, 14.06137039431157, 12.195036075036072, 8.026434676434675, 8.906169348367493, 5.115973266499582, 13.98192048749325, 10.831820728291325, 17.577406015037607, 8.669114000035055, 13.33933181369715, 15.621659630359332, 19.305818625818628, 13.025938148538764, 7.176687931951091, 10.962870234340826, 4.734584455791885, 13.298977166624223, 19.17661733003835, 8.924930820813177, 8.468935965994794, 11.051063903416848, 12.874076084733971, 10.636022408963582, 6.933880534670009, 13.518504273504275, 11.12347271923124, 8.63353809589104, 11.140193877330098, 7.496495726495728, 16.963824786324786, 9.877488328664802, 15.171075591075606, 17.388463069438306, 17.783953600578997, 14.069236181774883, 12.711502209520782, 10.897191697191696, 11.592906680522779, 11.017049062049056, 12.31013986013986, 7.662653457653458, 10.176969696969703, 14.285580391400828, 10.815226703121438, 11.52915562249928, 16.01184343434343, 11.828864468864476, 10.246918359418363, 4.995266955266955, 10.090240804946685, 4.929268626110731, 12.99835848362164, 8.43760990471517, 10.993901196842373]\n",
            "Rewards Average: 11.504850168381672\n",
            "Rewards Max: 19.305818625818628\n",
            "Rewards Min: 4.5455452390746505\n",
            "[6.089134199134199, 10.757509157509151, 8.50722590790702, 7.245714285714287, 9.219140989729222, 4.919950020739495, 9.125619994041047, 13.556492747019064, 2.856417112299466, 13.192891844997112, 12.066330891330882, 10.155509819557816, 14.112301587301586, 9.335465587044537, 13.822432730668034, 9.267549584918005, 8.257254582362942, 7.977699055330635, 10.633340213278295, 11.364444444444448, 13.58565177512545, 16.374212454212447, 9.60913681777459, 5.19905695611578, 15.217735042735033, 13.557079499447928, 9.366608035926925, 8.072471223307137, 9.305950370578858, 9.084252647936863, 4.076138584497717, 12.599542416647687, 8.487364243943192, 8.209607843137254, 5.40623716153128, 13.66240454076368, 6.734203574203574, 4.033502331002331, 9.782390932081338, 8.885633647398357, 10.946368327235206, 7.239754667417206, 10.11657166363049, 14.287130325814545, 6.999668174962293, 7.364413364413357, 14.522231313578079, 7.056702741702743, 4.82655129184541, 9.661086539879108, 12.739853235859435, 15.645945165945166, 6.486530691530692, 11.051971595655807, 12.959736842105263, 22.690178476494317, 10.248413155471981, 8.92333333333333, 13.26478663235629, 8.22933583959899, 9.020588235294118, 7.677688867719828, 13.885442072547342, 7.001699346405231, 13.199321789321788, 13.548444596339339, 8.948727880306832, 13.347102995044175, 8.021734143049935, 9.014327485380118, 9.501791979949875, 12.053068701164674, 11.577390411446139, 11.341991663307455, 8.0993894993895, 6.2325863678804865, 8.120002658160553, 11.152564102564108, 7.221899766899765, 14.362398190045253, 7.474931691975033, 10.851817188659298, 8.21905982905983, 21.578169934640542, 5.513045112781956, 15.697922665569731, 13.683968074409247, 12.30293884220354, 7.733333333333335, 2.508235294117647, 12.320970336852692, 9.44869566106408, 6.2499567099567095, 9.02497076023392, 11.296969696969697, 20.438855850032336, 17.700904183535773, 14.998571428571424, 12.260000000000005, 3.5312324929971988]\n",
            "Rewards Average: 10.251348780717047\n",
            "Rewards Max: 22.690178476494317\n",
            "Rewards Min: 2.508235294117647\n",
            "[6.464678362573099, 10.441269841269847, 13.892439577965893, 14.644047619047626, 15.932528190763474, 6.877908232118758, 12.101161616161617, 10.113795093795087, 5.2617027417027415, 22.91830409356724, 14.812612093788566, 10.737843147564512, 19.675099447336297, 10.198687782805429, 8.228403562381892, 6.9314027149321245, 9.696904761904765, 10.394581909318763, 13.16751461988305, 13.514607446217358, 7.897092994724573, 8.296703296703301, 13.582766955266957, 18.74900093370683, 7.41970310391363, 4.92988566988567, 5.244134199134198, 16.93943191311613, 10.824463118580768, 13.847415786827538, 22.834785915838548, 22.787769877181614, 8.364891774891776, 17.997982210428034, 8.753015873015872, 11.219508824508832, 11.961955340840793, 6.342345758816346, 6.960476190476191, 5.28022311022311, 20.760113488163036, 5.014401279834716, 9.44432431345744, 13.61068376068377, 11.509390331890337, 8.89819141642671, 11.284336313840946, 20.12515873015874, 6.781241830065358, 9.970608309818838, 11.514049830458498, 20.829222648696327, 12.2006120023767, 11.173918128654984, 17.331543799337904, 22.18652661064426, 6.60375457875458, 8.847330827067667, 12.454047619047616, 20.853099794941908, 16.765503062469, 10.886134698634697, 8.24060690943044, 12.185138253559309, 17.604450549450537, 26.557400703871316, 21.18120915032679, 17.65031746031747, 11.791417004048593, 8.015009586217015, 25.110987945662863, 14.54390515315438, 10.134508367449547, 17.58224550818979, 7.958733488733491, 15.909942279942266, 20.77799145299143, 23.340304466775056, 10.95310776942356, 15.550255826045303, 9.783016983016992, 15.392528981166741, 6.724425770308124, 14.197019471756331, 14.593965647355732, 14.432460317460299, 18.745888054154285, 22.285565394357945, 15.739285356885992, 6.702942057942059, 15.338610735669564, 18.767491560726835, 14.273320559729234, 13.077101173680132, 7.201739241151006, 16.045555555555538, 4.082323455698068, 24.259264069264063, 12.247944862155391, 5.300744016905006]\n",
            "Rewards Average: 13.095619642191311\n",
            "Rewards Max: 26.557400703871316\n",
            "Rewards Min: 4.082323455698068\n",
            "[6.85258352758353, 8.826065162907271, 7.469063568010936, 25.787004615611437, 11.813467889179963, 10.709460276565537, 19.65492063492064, 24.825728715728708, 10.789365079365082, 25.91833630421867, 17.490220498455816, 14.724509803921572, 15.822698412698404, 4.327222222222222, 21.855267230189874, 13.556470588235303, 12.17514619883041, 20.57700854700852, 23.561182869418193, 20.969942776521737, 12.839070374070376, 16.108119658119644, 13.429358974358962, 22.88743525480364, 8.660564552406658, 6.9410407480376435, 14.75866339542811, 39.3615347191509, 38.58244200244204, 21.516916221033828, 28.280182040182005, 6.946084763190028, 19.64294524189258, 12.280252100840345, 22.863336244079267, 22.7741980894922, 4.743781512605041, 12.090407232945918, 10.687534031156325, 48.09893901420231, 20.20825476209377, 19.35159694107066, 8.950317460317466, 16.664778946543652, 14.170000000000018, 8.158268398268401, 7.819915852980867, 9.6327731092437, 18.321468443836856, 15.632144979203776, 9.645149236728185, 17.0088888888889, 9.265094970389093, 13.693888888888875, 4.377636738906089, 16.977068975056607, 7.651904761904763, 16.42111816288286, 8.13985435960668, 7.098888888888889, 18.93602339181289, 17.30288737578211, 17.89659565580618, 21.716299723668147, 19.88443007972416, 16.721666666666646, 12.592222222222215, 9.139472649813213, 11.38997531879887, 17.880314087357444, 12.10611332252199, 7.213631939955473, 6.388995726495727, 11.130334660179862, 30.679132673312303, 26.380132294606028, 8.025186136532886, 25.20959595959594, 22.558679050567537, 13.31768740031898, 25.544919473371493, 18.87904761904759, 8.376209150326801, 8.294750601995185, 54.07240829346094, 13.145317656199998, 23.558611563874717, 28.507402700498673, 22.722848654613326, 25.424486215538778, 32.88079476079478, 19.143067226890768, 26.076375787954767, 29.603960848233353, 11.223355212039433, 16.29681246857715, 14.842222222222228, 28.45111111111112, 39.96797139908601, 21.83201837378307]\n",
            "Rewards Average: 17.457005515330906\n",
            "Rewards Max: 54.07240829346094\n",
            "Rewards Min: 4.327222222222222\n",
            "[6.078970049558291, 7.297380952380952, 20.2814561290103, 34.2629078351989, 15.661253641439396, 32.19692307692304, 9.4570868347339, 15.356428571428584, 16.075169295788488, 13.59353383458648, 11.35148097516518, 6.801372067904576, 19.24133089133092, 11.264704184704193, 17.665167084377593, 9.132963290579388, 12.449842379842378, 28.224875679875705, 6.970818713450292, 16.271154825566565, 17.44714727996463, 23.087134502923927, 41.54986443381187, 31.430329180623193, 49.677625918972566, 4.713333333333333, 17.63786158286157, 32.33087465166424, 20.477113997114024, 5.5248689614479085, 48.290125558995335, 18.8565073815074, 27.87413823019087, 8.756075977685885, 19.718487566784802, 33.633798812127, 13.769700230969578, 19.354700387331928, 13.059986504723359, 26.66976190476187, 15.295438417791338, 28.757957094505024, 29.82910881687976, 11.91549783549783, 41.68815596684018, 34.829988357016276, 14.108109980834431, 20.603537018536993, 34.700271132376415, 15.967222222222198, 15.176320346320344, 15.710317460317457, 59.66308550410721, 37.42890350877197, 4.772921522921525, 10.395910810895332, 47.281086048454426, 13.314469261388764, 40.24813976872806, 11.005800020413014, 39.0863909774436, 14.600919472684165, 20.87320901320899, 16.897774739880006, 36.28201198698105, 29.955002139243625, 29.450108850557683, 17.2184913263551, 15.859419486602134, 56.123779880250574, 24.965982905982905, 29.536951871657738, 14.897807486631043, 41.90935145264079, 7.901111111111115, 16.64043884220355, 17.14486596736596, 38.19577030812331, 57.974102564102495, 38.15638478480585, 42.19245319180296, 18.890651897122485, 11.941904761904754, 73.29740323402092, 24.121721951938643, 6.793195583226544, 34.09099832915615, 13.752719298245601, 4.900952380952381, 6.6055648926237165, 37.34119047619048, 13.07442545758335, 20.230160395160407, 6.471532035216247, 19.607598653496463, 66.0536017577194, 42.39348518354711, 3.0259958720330236, 14.875769816698616, 45.89686507936522]\n",
            "Rewards Average: 23.554145632222955\n",
            "Rewards Max: 73.29740323402092\n",
            "Rewards Min: 3.0259958720330236\n",
            "[59.01635966716458, 71.36149891388273, 23.139204383507764, 38.33394516731971, 3.738840571193512, 11.709081035923123, 24.84854784018869, 66.37357142857168, 50.73179615705926, 22.945277777777765, 14.863642137016765, 12.584682539682552, 7.063333333333335, 37.96479125621538, 14.818320234976579, 9.459398496240619, 27.180564468211536, 26.77537037523876, 36.32201754385973, 27.391746031746006, 66.6688894181155, 11.98550543024227, 45.634276878951916, 7.518487735175037, 21.975808080808136, 10.721179826365582, 69.10637395628079, 23.914635230610433, 22.73586008762479, 7.778245614035089, 62.22104012258798, 11.659051907844482, 8.2692880219196, 12.238943839098631, 15.433788883974653, 29.11856592152255, 9.63551587301587, 36.40178998847727, 19.469067700445418, 33.47371553884714, 18.064390463337844, 20.264789915966375, 83.2702020202022, 33.04044916212092, 77.40285714285662, 23.432476383265843, 7.9032476295634195, 12.781483745013144, 14.456198433489444, 27.43740724359913, 69.68896874539281, 57.6487734487735, 54.33187978687933, 32.35418300653604, 7.051456582633054, 65.11060468942802, 56.265226703121435, 25.84555555555555, 34.141351625670545, 43.43679425837311, 91.11441669441672, 28.180952380952323, 28.886666666666727, 68.88444699091768, 13.571946778711473, 79.1849999999999, 31.961800775763656, 44.412439923337836, 15.962781064824432, 75.93754142658148, 66.11165969737151, 3.9719221129747444, 99.071454627244, 19.764367971210046, 37.15888888888896, 66.19441880101301, 25.33218281718276, 28.338699078699065, 17.447090066795965, 26.415126050420128, 44.751991207966256, 19.11909812409812, 56.06324561403514, 138.6653508771931, 129.01318148312015, 20.02784313725491, 9.405173160173158, 18.846344537815114, 33.291152882205566, 55.98430166324909, 28.857136752136824, 14.514999999999983, 22.22068922305765, 7.07204059098796, 78.42069740716796, 36.22294117647068, 57.57817460317454, 70.98414529914476, 9.382916038210157, 51.545923453911115]\n",
            "Rewards Average: 36.46381466002176\n",
            "Rewards Max: 138.6653508771931\n",
            "Rewards Min: 3.738840571193512\n",
            "[68.10009214211999, 14.060582693879887, 37.865580618212206, 29.51543008075517, 13.244006432164316, 38.18586367880485, 41.112493734335736, 83.14917637917613, 44.44765001768097, 16.159563754099352, 41.55096122659587, 75.02571625141266, 27.785524114688226, 176.8525205204138, 65.5998186643078, 71.96086834733892, 70.03896825396835, 56.86127082411918, 33.023112192245314, 25.249806567701253, 46.08652680652684, 17.0930035023456, 64.23207367795578, 7.2699346405228775, 56.69651583710428, 68.85960317460264, 178.09804648567865, 18.069747899159665, 40.792282488752925, 29.8832777456462, 102.9327777777772, 94.14557442557478, 33.123296476485294, 8.041057697032933, 68.46763375513382, 26.909218500797486, 20.83160614239565, 44.63684210526311, 89.97370934019506, 38.84888895418304, 63.14128319049353, 20.102893546082413, 120.35705882352866, 4.5809523809523816, 28.136287666581747, 9.500612230906347, 82.82325233418126, 32.076226126814355, 9.70428571428572, 55.910737800211756, 31.480552503052458, 25.53738562091504, 71.33156862745062, 70.79457320457334, 42.49858395989984, 61.111309523809474, 62.39349206349202, 79.96073959408295, 22.046959151959175, 104.98741363212051, 90.06901469359673, 159.1230368814205, 71.94001443001449, 61.220084703737996, 26.279952369611767, 117.23345543345702, 27.496904761904695, 128.0327327935226, 60.46786451422681, 54.33411587638362, 67.02873359870227, 92.02293916609769, 58.800167084377286, 63.284152046783746, 11.802464006581628, 152.69563534504746, 19.679758085052217, 17.94712714186399, 66.02601065601044, 7.408227299016774, 81.13330634277995, 184.05817927170855, 77.60898078529694, 61.22975367941347, 9.352578147686502, 55.67547201336662, 47.496248653307696, 46.75462922010916, 58.56514411027577, 63.6885508935511, 92.00379834303976, 56.4072649572652, 9.288467246547745, 36.55357824150391, 4.746031746031747, 105.94926338281569, 225.31696969696617, 111.50822288822272, 97.59540015539989, 5.5271914188322855]\n",
            "Rewards Average: 58.666082137040796\n",
            "Rewards Max: 225.31696969696617\n",
            "Rewards Min: 4.5809523809523816\n",
            "[37.86057889822596, 197.32443850267344, 121.30761904761958, 81.62088669500422, 47.30998778998793, 55.28117647058803, 88.27475226527893, 5.832008547008547, 15.143214762533665, 25.05704067861966, 174.12794901692837, 99.13763061499893, 17.302692307692276, 116.25890331890352, 8.874843391902216, 39.41477540003845, 66.54229544449326, 61.93030557677647, 9.303046757164406, 87.60030597885067, 31.30230158730157, 81.79806106174507, 24.56918570937143, 24.98858585858583, 53.28775271512102, 112.25043328016976, 38.65200466200459, 60.44949494949527, 66.25700854700852, 70.23527777777768, 71.24725723536848, 42.60775127864918, 58.977379336349905, 17.53134920634918, 7.839737660000811, 240.78678514467953, 5.1224630202958386, 144.83322928533408, 156.05951571991912, 61.578280542986775, 93.1900527250522, 104.70003251134862, 47.56887556887544, 100.57283549783558, 133.9739303506968, 67.59677365204476, 161.70512820512843, 166.72460317460195, 72.48249299719923, 232.9750208855442, 55.64428571428591, 5.753333333333333, 11.882759170653888, 32.740776450931214, 71.54273236567357, 41.91716374269019, 99.57038076474615, 12.704544695071029, 127.07111111111097, 33.013938617282236, 95.76069364588028, 94.30604237867415, 203.84259347120158, 111.3680720414774, 146.64299393417008, 8.089523809523817, 104.22119674185453, 145.26625668449157, 64.67799452439527, 134.46420634920554, 168.7624183006551, 140.39628384687273, 97.88440845703998, 123.77940115440025, 95.07394122561205, 111.30216810966868, 498.8982099273264, 23.848995414630075, 59.975529743734384, 110.20187596201538, 26.047410367410354, 64.26146026831789, 31.585229986731502, 73.91126984126977, 80.74388506917921, 24.725088100753783, 189.85497661265782, 40.04098079423136, 101.17225957049469, 95.53393939393992, 137.82144469565526, 56.30285810680528, 57.76837285521477, 101.41522556391054, 266.8675793650757, 111.71296324590448, 54.05433806991075, 74.70841368126187, 88.59066933066921, 66.79785714285711]\n",
            "Rewards Average: 86.81888105362991\n",
            "Rewards Max: 498.8982099273264\n",
            "Rewards Min: 5.1224630202958386\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-57c137f88b3f>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtraining_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EVALUATE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-cb994fa49da9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, env, num_episodes)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Always use the learned policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-cb994fa49da9>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0maction_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_index\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-cb994fa49da9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: display stats from the training_tracker variable\n",
        "\n",
        "import numpy as np\n",
        "# Print training statistics\n",
        "print(\"Training Tracker:\")\n",
        "for i, (phase, rewards) in enumerate(training_tracker):\n",
        "    print(f\"Phase {phase}, Episode {i + 1}:\")\n",
        "    print(f\"  Average Reward: {np.average(rewards)}\")\n",
        "    print(f\"  Maximum Reward: {max(rewards)}\")\n",
        "    print(f\"  Minimum Reward: {min(rewards)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxqJBiSyLcpo",
        "outputId": "bc902995-af64-4996-f9d5-fc9211c64a75"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Tracker:\n",
            "Phase TRAIN, Episode 1:\n",
            "  Average Reward: 8.549120289763557\n",
            "  Maximum Reward: 15.411320346320341\n",
            "  Minimum Reward: 2.999908424908425\n",
            "Phase TRAIN, Episode 2:\n",
            "  Average Reward: 9.958879572955425\n",
            "  Maximum Reward: 19.26987007488555\n",
            "  Minimum Reward: 4.9159884559884555\n",
            "Phase TRAIN, Episode 3:\n",
            "  Average Reward: 11.504850168381672\n",
            "  Maximum Reward: 19.305818625818628\n",
            "  Minimum Reward: 4.5455452390746505\n",
            "Phase TRAIN, Episode 4:\n",
            "  Average Reward: 10.251348780717047\n",
            "  Maximum Reward: 22.690178476494317\n",
            "  Minimum Reward: 2.508235294117647\n",
            "Phase TRAIN, Episode 5:\n",
            "  Average Reward: 13.095619642191311\n",
            "  Maximum Reward: 26.557400703871316\n",
            "  Minimum Reward: 4.082323455698068\n",
            "Phase TRAIN, Episode 6:\n",
            "  Average Reward: 17.457005515330906\n",
            "  Maximum Reward: 54.07240829346094\n",
            "  Minimum Reward: 4.327222222222222\n",
            "Phase TRAIN, Episode 7:\n",
            "  Average Reward: 23.554145632222955\n",
            "  Maximum Reward: 73.29740323402092\n",
            "  Minimum Reward: 3.0259958720330236\n",
            "Phase TRAIN, Episode 8:\n",
            "  Average Reward: 36.46381466002176\n",
            "  Maximum Reward: 138.6653508771931\n",
            "  Minimum Reward: 3.738840571193512\n",
            "Phase TRAIN, Episode 9:\n",
            "  Average Reward: 58.666082137040796\n",
            "  Maximum Reward: 225.31696969696617\n",
            "  Minimum Reward: 4.5809523809523816\n",
            "Phase TRAIN, Episode 10:\n",
            "  Average Reward: 86.81888105362991\n",
            "  Maximum Reward: 498.8982099273264\n",
            "  Minimum Reward: 5.1224630202958386\n"
          ]
        }
      ]
    }
  ]
}