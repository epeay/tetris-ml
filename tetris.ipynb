{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1bLePsLvBZHQimixTu8nfr0Mb3QruvANA",
      "authorship_tag": "ABX9TyMn9l9FyOnyz79t+A8JtEsQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epeay/tetris-ml/blob/main/tetris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/tensor-logs/runs\""
      ],
      "metadata": {
        "id": "KBWTAAbe2WU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "##################\n",
        "# Environment Prep\n",
        "##################\n",
        "import importlib, sys\n",
        "if local_libs not in sys.path:\n",
        "    sys.path = local_libs + sys.path\n",
        "def auto_pip(libraries):\n",
        "    \"\"\" Invokes pip if needed. Saves time if not. \"\"\"\n",
        "    import importlib\n",
        "    try:\n",
        "        for library in libraries:\n",
        "            importlib.import_module(library)\n",
        "    except ImportError:\n",
        "        !pip install {\" \".join(libraries)}\n",
        "# avoids invoking pip unless we need it\n",
        "auto_pip([\"gymnasium\"])\n",
        "# Pull latest changes from local library\n",
        "######################\n",
        "# End environment prep\n",
        "######################\n",
        "\n",
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pdb\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Episode = One tetris game\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ActionFeedback:\n",
        "    def __init__(self, valid_action=False):\n",
        "        self.valid_action = valid_action\n",
        "        self.is_predict = False\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"ActionFeedback(valid_action={self.valid_action}, is_predict={self.is_predict})\"\n",
        "\n",
        "class TetrominoPiece:\n",
        "\n",
        "    BLOCK = '▆'\n",
        "\n",
        "    def __init__(self, shape:int, patterns):\n",
        "        self.shape:int = shape\n",
        "        self.pattern_list = patterns\n",
        "        self.pattern = patterns[0]\n",
        "        self.rot = 0\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"TetrominoPiece(shape={Tetrominos.shape_name(self.shape)}, rot={self.rot*90}, pattern= {self.printable_pattern(oneline=True)})\"\n",
        "\n",
        "    def printable_pattern(self, oneline=False):\n",
        "        ret = []\n",
        "        pattern = self.get_pattern()\n",
        "        for i, row in enumerate(pattern):\n",
        "            row_str = \" \".join([str(c) for c in row])\n",
        "            ret.append(row_str)\n",
        "\n",
        "            if not oneline:\n",
        "                ret.append(\"\\n\")\n",
        "            else:\n",
        "                if i < len(pattern)-1:\n",
        "                    ret.append(\" / \",)\n",
        "        ret = \"\".join(ret).replace('1', TetrominoPiece.BLOCK).replace('0', '_')\n",
        "        return \"\".join(ret)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"shape\": self.shape,\n",
        "            \"pattern\": self.pattern\n",
        "        }\n",
        "\n",
        "    def get_pattern(self):\n",
        "        return self.pattern\n",
        "\n",
        "    def rotate(self):\n",
        "        \"\"\"Rotates IN PLACE, and returns the new pattern\"\"\"\n",
        "        self.rot = (self.rot + 1) % 4\n",
        "        self.pattern = self.pattern_list[self.rot]\n",
        "        return self.pattern\n",
        "\n",
        "    def get_height(self):\n",
        "        return len(self.get_pattern())\n",
        "\n",
        "    def get_width(self):\n",
        "        return max([len(x) for x in self.get_pattern()])\n",
        "\n",
        "    def get_bottom_offsets(self):\n",
        "        \"\"\"\n",
        "        For each column in the shape, returns the gap between the bottom of\n",
        "        the shape (across all columns) and the bottom of the shape in that\n",
        "        column.\n",
        "\n",
        "        Returned values in the list would expect to contain at least one 0, and\n",
        "        no values higher than the height of the shape.\n",
        "\n",
        "        For example, an S piece:\n",
        "        _ X X\n",
        "        X X _\n",
        "\n",
        "        Would have offsets [0, 0, 1] in this current rotation. This method is\n",
        "        used in determining if a piece will fit at a certain position\n",
        "        in the board.\n",
        "        \"\"\"\n",
        "        pattern = self.get_pattern()\n",
        "        # pdb.set_trace()\n",
        "        ret = [len(pattern)+1 for x in range(len(pattern[0]))]\n",
        "        # Iterates rows from top, down\n",
        "        for ri in range(len(pattern)):\n",
        "            # Given a T shape:\n",
        "            # X X X\n",
        "            # _ X _\n",
        "            # Start with row [X X X] (ri=0, offset=1)\n",
        "            row = pattern[ri]\n",
        "            # print(f\"Testing row {row} at index {ri}\")\n",
        "            for ci, col in enumerate(row):\n",
        "                if col == 1:\n",
        "                    offset = len(pattern) - ri - 1\n",
        "                    ret[ci] = offset\n",
        "\n",
        "            # Will return [1, 0, 1] for a T shape\n",
        "\n",
        "        if max(ret) >= len(pattern):\n",
        "          print(f\"Pattern:\")\n",
        "          print(pattern)\n",
        "          print(f\"Bottom Offsets: {ret}\")\n",
        "          print(f\"Shape: {self.shape}\")\n",
        "          raise ValueError(\"Tetromino pattern has incomplete bottom offsets\")\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def get_top_offsets(self):\n",
        "        \"\"\"\n",
        "        Returns the height of the shape at each column.\n",
        "\n",
        "        For example, an S piece:\n",
        "        _ X X\n",
        "        X X _\n",
        "\n",
        "        Would have offsets [1, 2, 2] in this current rotation. This provides\n",
        "        guidance on how to update the headroom list.\n",
        "\n",
        "        Ideally we should cache this.\n",
        "        \"\"\"\n",
        "        pattern = self.get_pattern()\n",
        "        ret = [0 for x in len(pattern[0])]\n",
        "        for ri, row in enumerate(range(pattern, )):\n",
        "            for col in pattern[row]:\n",
        "                if pattern[row][col] == 1:\n",
        "                    ret[col] = max(ret[col], row)\n",
        "        return ret\n",
        "\n",
        "\n",
        "class Tetrominos:\n",
        "    O = 1\n",
        "    I = 2\n",
        "    S = 3\n",
        "    Z = 4\n",
        "    T = 5\n",
        "    J = 6\n",
        "    L = 7\n",
        "    DOT = 8\n",
        "    USCORE = 9\n",
        "\n",
        "    base_patterns = {\n",
        "        # X X\n",
        "        # X X\n",
        "        O: np.array([[1, 1], [1, 1]]),\n",
        "\n",
        "        # X X X X\n",
        "        I: np.array([[1, 1, 1, 1]]),\n",
        "\n",
        "        # _ X X\n",
        "        # X X _\n",
        "        S: np.array([[0, 1, 1], [1, 1, 0]]),\n",
        "        Z: np.array([[1, 1, 0], [0, 1, 1]]),\n",
        "        T: np.array([[1, 1, 1], [0, 1, 0]]),\n",
        "        J: np.array([[1, 0, 0], [1, 1, 1]]),\n",
        "        L: np.array([[0, 0, 1], [1, 1, 1]]),\n",
        "        DOT: np.array([[1]]),\n",
        "        USCORE: np.array([[1,1]])\n",
        "    }\n",
        "\n",
        "    # Stores patterns for each tetromino, at each rotation\n",
        "    cache = {}\n",
        "\n",
        "    def num_tetrominos():\n",
        "        return len(Tetrominos.base_patterns.keys())\n",
        "\n",
        "    @staticmethod\n",
        "    def shape_name(shape):\n",
        "        if shape == Tetrominos.O:\n",
        "            return \"O\"\n",
        "        elif shape == Tetrominos.I:\n",
        "            return \"I\"\n",
        "        elif shape == Tetrominos.S:\n",
        "            return \"S\"\n",
        "        elif shape == Tetrominos.Z:\n",
        "            return \"Z\"\n",
        "        elif shape == Tetrominos.T:\n",
        "            return \"T\"\n",
        "        elif shape == Tetrominos.J:\n",
        "            return \"J\"\n",
        "        elif shape == Tetrominos.L:\n",
        "            return \"L\"\n",
        "        elif shape == Tetrominos.DOT:\n",
        "            return \"DOT\"\n",
        "        elif shape == Tetrominos.USCORE:\n",
        "            return \"USCORE\"\n",
        "        else:\n",
        "            raise ValueError(\"Invalid shape\")\n",
        "\n",
        "    @staticmethod\n",
        "    def make(shape):\n",
        "        \"\"\"\n",
        "        shape:\n",
        "        \"\"\"\n",
        "        if not Tetrominos.cache:\n",
        "            for shape, pattern in Tetrominos.base_patterns.items():\n",
        "                Tetrominos.cache[shape] = [\n",
        "                    pattern,\n",
        "                    np.rot90(pattern),\n",
        "                    np.rot90(pattern, 2),\n",
        "                    np.rot90(pattern, 3)\n",
        "                ]\n",
        "\n",
        "\n",
        "        if shape not in Tetrominos.base_patterns.keys():\n",
        "            raise ValueError(\"Invalid shape\")\n",
        "\n",
        "        return TetrominoPiece(shape, Tetrominos.cache[shape])\n",
        "\n",
        "Tetrominos.std_bag = [\n",
        "    Tetrominos.O,\n",
        "    Tetrominos.I,\n",
        "    Tetrominos.S,\n",
        "    Tetrominos.Z,\n",
        "    Tetrominos.T,\n",
        "    Tetrominos.J,\n",
        "    Tetrominos.L\n",
        "]\n",
        "\n",
        "class TetrisBoard:\n",
        "\n",
        "    BLOCK = '▆'\n",
        "\n",
        "    def __init__(self, matrix, height):\n",
        "        self.play_height = height\n",
        "        self.height = len(matrix)\n",
        "        self.width = len(matrix[0])\n",
        "        self.board = matrix\n",
        "\n",
        "    def reset(self):\n",
        "        self.board.fill(0)\n",
        "        self.piece = None\n",
        "\n",
        "    def remove_tetris(self):\n",
        "        to_delete = []\n",
        "        for r, row in enumerate(self.board):\n",
        "            if sum(row) == self.width:\n",
        "                to_delete.append(r)\n",
        "\n",
        "        if to_delete:\n",
        "          self.board = np.delete(self.board, to_delete, axis=0)\n",
        "          self.board.resize((self.height, self.width))\n",
        "          # pdb.set_trace()\n",
        "\n",
        "        return len(to_delete)\n",
        "\n",
        "    def place_piece(self, piece:TetrominoPiece, logical_coords):\n",
        "        \"\"\"\n",
        "        Places a piece at the specified column. Dynamically calculates correct\n",
        "        height for the piece.\n",
        "\n",
        "        piece: a TetrominoPiece object\n",
        "        logical_coords: The logical row and column for the bottom left\n",
        "            of the piece's pattern\n",
        "        \"\"\"\n",
        "        pattern = piece.get_pattern()\n",
        "\n",
        "        lrow = logical_coords[0]\n",
        "        lcol = logical_coords[1]\n",
        "\n",
        "        p_height = piece.get_height()\n",
        "\n",
        "        for r in range(p_height):\n",
        "            pattern_row = pattern[len(pattern)-1-r]\n",
        "            board_row = self.board[lrow-1+r]\n",
        "\n",
        "            for i, c in enumerate(pattern_row):\n",
        "                # Iff c is 1, push it to the board\n",
        "                board_row[lcol-1+i] |= c\n",
        "\n",
        "\n",
        "    def find_logical_BL_placement(self, piece:TetrominoPiece, col):\n",
        "        \"\"\"\n",
        "        Assumes the piece fits on the board, horizontally. The piece WILL fit\n",
        "        vertically, as there are 4 empty rows at the top of the board, which if\n",
        "        utilized, trigger game over.\n",
        "\n",
        "        Returns the logical row and column of the bottom left corner of the\n",
        "        pattern, such that when placed, the piece will sit flush against existing\n",
        "        tower parts, and not exceed the max board height.\n",
        "\n",
        "        Given:\n",
        "        BOARD       PIECE\n",
        "        5 _ _ _ _\n",
        "        4 _ _ _ X\n",
        "        3 _ _ X X   X X X X\n",
        "        2 _ X X _\n",
        "        1 X X X X\n",
        "\n",
        "        Returns (5, 1)\n",
        "\n",
        "        Given:\n",
        "        BOARD       PIECE    COL\n",
        "        5 _ _ _ _\n",
        "        4 _ _ _ X\n",
        "        3 _ _ X X   X X X    1 (lcol 2)\n",
        "        2 _ X X _     X\n",
        "        1 X X X X\n",
        "\n",
        "        Returns (3, 1)\n",
        "\n",
        "        piece: a TetrominoPiece object\n",
        "        col: zero-index column to place the 0th column of the piece.\n",
        "        \"\"\"\n",
        "\n",
        "        pattern = piece.get_pattern()\n",
        "        bottom_offsets = np.array(piece.get_bottom_offsets())\n",
        "        # TODO don't calculate all bottoms because we don't need them all\n",
        "        board_heights = np.array(self.get_tops()[col:col+piece.get_width()])\n",
        "\n",
        "        # Given:\n",
        "        # BOARD       PIECE\n",
        "        # 5 _ _ _ _\n",
        "        # 4 _ _ _ X\n",
        "        # 3 _ _ X X   X X X X\n",
        "        # 2 _ X X _\n",
        "        # 1 X X X X\n",
        "        # Tops -> [1,2,3,4]\n",
        "        #\n",
        "        # The sideways I has bottom offsets [0,0,0,0]\n",
        "        # Start at min(board_tops)+1 and try to place the piece.\n",
        "        #\n",
        "        # If placing on row 2, the piece heights would be [2,2,2,2]g\n",
        "        # Board heights are [1,2,3,4], so this\n",
        "        # doesn't clear the board for all columns. Try placing on row 3.\n",
        "        # [3,3,3,3] > [1,2,3,4] ? False\n",
        "        # Try row 4... False. Try row 5...\n",
        "        # [5,5,5,5] > [1,2,3,4] ? True\n",
        "        # So we place the piece on row 5 (index 4)\n",
        "        #\n",
        "        # 5 X X X X\n",
        "        # 4 _ _ _ X\n",
        "        # 3 _ _ X X\n",
        "        # 2 _ X X _\n",
        "        # 1 X X X X\n",
        "        # (yes, this is a horrible move)\n",
        "\n",
        "        p_height = piece.get_height()\n",
        "        p_width = piece.get_width()\n",
        "        can_place = False\n",
        "\n",
        "        # TODO Pick better min test height\n",
        "        # If there's a very narrow, tall tower, and you're placing a flat I\n",
        "        # just to the left of it, you'll likely test placement for each level of\n",
        "        # the tower until the piece clears it.\n",
        "        for place_row in range(min(board_heights)+1, max(board_heights)+2):\n",
        "            # In the example, place_row would be 2...3...4...5\n",
        "\n",
        "            bottom_clears_board = all((bottom_offsets + place_row) > board_heights)\n",
        "            if bottom_clears_board:\n",
        "                break\n",
        "\n",
        "        return (place_row, col+1)\n",
        "\n",
        "    @staticmethod\n",
        "    def render_state(board, pattern, bl_coords, color=True):\n",
        "        board = board.copy()\n",
        "\n",
        "        # Highlight tiles where the last piece was played\n",
        "        lrow, lcol = bl_coords\n",
        "\n",
        "        p_height = len(pattern)\n",
        "        output = False\n",
        "\n",
        "        for r in range(p_height):\n",
        "            pattern_row = pattern[len(pattern)-1-r]\n",
        "            board_row = board[lrow-1+r]\n",
        "\n",
        "            for i, c in enumerate(pattern_row):\n",
        "                # Iff c is 1, push it to the board\n",
        "                if c == 1:\n",
        "                    board_row[lcol-1+i] = 2\n",
        "\n",
        "        print(f\"{(len(board) -i) % 10} \", end=\"\")\n",
        "        for i, row in enumerate(reversed(board)):\n",
        "            if sum(row) == 0 and not output:\n",
        "                continue\n",
        "            else:\n",
        "                output = True\n",
        "\n",
        "            for cell in row:\n",
        "                if cell == 2:\n",
        "                    print(f\"\\033[36m{TetrisBoard.BLOCK}\\033[0m\", end=' ')\n",
        "                elif cell == 1:\n",
        "                    print(TetrisBoard.BLOCK, end=' ')\n",
        "                else:\n",
        "                    print('_', end=' ')\n",
        "            print()\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        output = False\n",
        "        for i, row in enumerate(reversed(self.board)):\n",
        "            # if sum(row) == 0 and not output:\n",
        "            #     continue\n",
        "            # else:\n",
        "            #     output = True\n",
        "\n",
        "            output = True\n",
        "\n",
        "            print(f\"{(self.height -i) % 10} \", end=\"\")\n",
        "            for cell in row:\n",
        "                if cell == 1:\n",
        "                    print(TetrisBoard.BLOCK, end=' ')\n",
        "                else:\n",
        "                    empty = '_'\n",
        "                    if (self.height -i) > 20:\n",
        "                        empty = 'X'\n",
        "\n",
        "                    print(empty, end=' ')\n",
        "            print()\n",
        "\n",
        "        if not output:\n",
        "            print(\"<<EMPTY BOARD>>\")\n",
        "\n",
        "\n",
        "\n",
        "    def get_tops(self):\n",
        "        \"\"\"\n",
        "        Gets the height of each column on the board.\n",
        "        This is gonna be inefficient for now.\n",
        "\n",
        "        A board with only an I at the left side would return [4, 0, 0, ...]\n",
        "        \"\"\"\n",
        "        tops = [0 for _ in range(self.width)]\n",
        "        for r, row in enumerate(self.board):\n",
        "            if sum(row) == 0:\n",
        "                break\n",
        "\n",
        "            for col, val in enumerate(row):\n",
        "                if val == 1:\n",
        "                    tops[col] = r+1\n",
        "\n",
        "        return tops\n",
        "\n",
        "\n",
        "class TetrisGameRecord:\n",
        "    def __init__(self):\n",
        "        self.id = None  # Populated later\n",
        "        self.moves = 0\n",
        "        self.invalid_moves = 0\n",
        "        self.lines_cleared = 0\n",
        "        self.cleared_by_size = {\n",
        "            1: 0,\n",
        "            2: 0,\n",
        "            3: 0,\n",
        "            4: 0\n",
        "        }\n",
        "        self.boards = []\n",
        "        self.pieces = []\n",
        "        self.placements = []  # Logical coords of BL corner of piece pattern\n",
        "        self.rewards = []\n",
        "        self.outcome = []\n",
        "        self.cumulative_reward = 0\n",
        "        self.is_predict = []\n",
        "        self.episode_start_time = time.monotonic_ns()\n",
        "        self.episode_end_time = None\n",
        "        self.duration_ns = None\n",
        "        self.agent_info = {}\n",
        "        self.logg = None\n",
        "\n",
        "\n",
        "\n",
        "class TetrisEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(TetrisEnv, self).__init__()\n",
        "        self.board_height = 20\n",
        "        self.board_width = 10\n",
        "        self.current_piece = None\n",
        "        self.pieces = Tetrominos()\n",
        "        self.reward_history = deque(maxlen=10)\n",
        "        self.record = TetrisGameRecord()\n",
        "        self.piece_bag = Tetrominos.std_bag\n",
        "\n",
        "        # Indexes  0-19 - The visible playfield\n",
        "        #         20-23 - Buffer for the next piece to sit above the board\n",
        "        self.state = np.zeros((self.board_height + 4, self.board_width), dtype=int)\n",
        "\n",
        "        # Creates a *view* from the larger state\n",
        "        self.current_piece_rows = self.state[20:24]\n",
        "        self.board = TetrisBoard(self.state, 20)\n",
        "\n",
        "        # Action space: tuple (column, rotation)\n",
        "        self.action_space = spaces.MultiDiscrete([self.board_width, 4])\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board.reset()\n",
        "        self.current_piece = self._get_random_piece()\n",
        "        self.record = TetrisGameRecord()\n",
        "        return self._get_board_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        # ([0-9], [0-3])\n",
        "        col, rotation = action\n",
        "        lcol = col + 1\n",
        "\n",
        "        info = ActionFeedback()\n",
        "\n",
        "        # Rotate the piece to the desired rotation\n",
        "        for _ in range(rotation):\n",
        "            self.current_piece.rotate()  # Rotates IN PLACE\n",
        "\n",
        "        # Clear the area above the visible board. If this range is used during\n",
        "        # piece placement, the game is over.\n",
        "        self.board.board[-4:].fill(0)\n",
        "\n",
        "        # Check for right-side overflow\n",
        "        # Given a horizontal I piece on col 0\n",
        "        # right_lcol would be 4. The piece would occupy lcolumns 1-4.\n",
        "        right_lcol = lcol-1 + self.current_piece.get_width()\n",
        "        if right_lcol > self.board_width:\n",
        "            # Ignore this action and try again.\n",
        "            #\n",
        "            # For example, a location is chosen which extends\n",
        "            # the piece over the edge of the board.\n",
        "            done = False\n",
        "            info.valid_action = False\n",
        "            self.current_piece.rot = 0\n",
        "            self.record.invalid_moves += 1\n",
        "            reward = -1\n",
        "\n",
        "            return self._get_board_state(), reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "        info.valid_action = True\n",
        "        lcoords = None\n",
        "\n",
        "        lcoords = self.board.find_logical_BL_placement(self.current_piece, col)\n",
        "        self.board.place_piece(self.current_piece, lcoords)\n",
        "\n",
        "\n",
        "        self.render()\n",
        "\n",
        "\n",
        "\n",
        "        self.record.moves += 1\n",
        "        self.record.boards.append(self.board.board.copy())\n",
        "        self.record.pieces.append(self.current_piece.to_dict())\n",
        "        self.current_piece.rot = 0\n",
        "\n",
        "\n",
        "        # If any of the top four rows were used -- Game Over\n",
        "        if np.any(self.board.board[-4:]):\n",
        "            # Game Over\n",
        "            done = True\n",
        "            reward = -1\n",
        "\n",
        "            self.record.rewards.append(reward)\n",
        "            self.record.placements.append(None)\n",
        "            self.record.cumulative_reward += reward\n",
        "            self.reward_history.append(reward)\n",
        "\n",
        "            self.close_episode()\n",
        "\n",
        "            self.board.render()\n",
        "\n",
        "\n",
        "            return self._get_board_state(), reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "        reward = self.board_height - lcoords[0]\n",
        "        print(f\"Reward is {reward} for coords {lcoords}\")\n",
        "\n",
        "\n",
        "        # reward = self._calculate_reward()\n",
        "        done = False\n",
        "\n",
        "        self.record.rewards.append(reward)\n",
        "        self.record.placements.append(lcoords)\n",
        "        self.record.cumulative_reward += reward\n",
        "        self.reward_history.append(reward)\n",
        "\n",
        "        # Huzzah!\n",
        "        lines_gone = self.board.remove_tetris()\n",
        "        if lines_gone > 0:\n",
        "            self.record.lines_cleared += 1\n",
        "            self.record.cleared_by_size[lines_gone] += 1\n",
        "\n",
        "        reward += lines_gone * 100\n",
        "\n",
        "        print(f\"Reward is {reward} for coords {lcoords}\")\n",
        "        if lines_gone > 0:\n",
        "            print(f\"AND CLEARING {lines_gone} LINES\")\n",
        "            print(\"----------------------\")\n",
        "\n",
        "        # Prep for next move\n",
        "        self.current_piece = self._get_random_piece()\n",
        "        next_state = self._get_board_state()\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "\n",
        "    def close_episode(self):\n",
        "        \"\"\"\n",
        "        Wraps up episode stats. Public method is available for agent to call\n",
        "        if needed.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.record.episode_end_time:\n",
        "            # Already closed\n",
        "            return\n",
        "\n",
        "        self.record.episode_end_time = time.monotonic_ns()\n",
        "        self.record.duration_ns = self.record.episode_end_time - self.record.episode_start_time\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        self.board.render()\n",
        "\n",
        "    def _get_random_piece(self):\n",
        "        return self.pieces.make(random.choice(self.piece_bag))\n",
        "\n",
        "    def _is_valid_action(self, piece, lcol):\n",
        "        piece = self.current_piece\n",
        "\n",
        "        if lcol < 1 or lcol > self.board_width:\n",
        "            return False\n",
        "\n",
        "        # An O piece on col 1 would occupy cols 1-2\n",
        "        if lcol + piece.get_width() -1 > self.board_width:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "\n",
        "        # Evaluate line pack\n",
        "        # Packed lines produces a higher score\n",
        "        # Big narrow tower would produce a low score\n",
        "        active_lines = 0\n",
        "        board_tiles = 0\n",
        "        lines_cleared = 0\n",
        "        board = self.board.board\n",
        "\n",
        "        for row in self.board.board:\n",
        "            row_sum = sum(row)\n",
        "            board_tiles += row_sum\n",
        "            if row_sum == 0:\n",
        "                continue\n",
        "\n",
        "            active_lines += 1\n",
        "            if row_sum == self.board.width:\n",
        "                lines_cleared += 1\n",
        "\n",
        "        if active_lines == 0:\n",
        "            return 0\n",
        "\n",
        "        # Simulating an extra 10 packed tiles per line cleared\n",
        "        line_clear_bonus = 10\n",
        "\n",
        "        # Narrow towers get lower rewards. Starting with row 3, every row that\n",
        "        # has < 50% pack, gets a penalty of this many additional empty tiles when\n",
        "        # calculating the pack score.\n",
        "        sharp_tower_penalty = 3\n",
        "        sharp_tower_pack_min = 0.5\n",
        "        underpacked_lines = 0\n",
        "\n",
        "        line_pack_pct = [(sum(x) / self.board.width) for x in self.board.board]\n",
        "        high_tower_penalty = 0\n",
        "        for pct in line_pack_pct[3:active_lines]:\n",
        "            if pct < sharp_tower_pack_min:\n",
        "                underpacked_lines += 1\n",
        "\n",
        "        high_tower_penalty = underpacked_lines * sharp_tower_penalty\n",
        "\n",
        "        line_score = (board_tiles+(10*lines_cleared)) / float(self.board_width * active_lines + high_tower_penalty)\n",
        "\n",
        "        line_pack_pct = [sum(x) / self.board.width for x in self.board.board]\n",
        "\n",
        "        reward = line_score  # That's all for now\n",
        "        return reward\n",
        "\n",
        "    def _get_board_state(self):\n",
        "\n",
        "        self.current_piece.get_pattern()\n",
        "        self.board.place_piece(self.current_piece, (21, 1))\n",
        "\n",
        "        return self.state[np.newaxis, :, :]\n",
        "\n",
        "        # Construct a state representation with the board and current piece\n",
        "        board_state = np.expand_dims(self.board.board, axis=0)\n",
        "\n",
        "        # Create a piece state with just the current piece in the center of its own grid\n",
        "        piece_state = np.zeros((self.board_height, self.board_width), dtype=int)\n",
        "        piece_pattern = self.current_piece.get_pattern()\n",
        "\n",
        "        piece_height, piece_width = piece_pattern.shape\n",
        "        start_row = (self.board_height - piece_height) // 2\n",
        "        start_col = (self.board_width - piece_width) // 2\n",
        "\n",
        "        piece_state[start_row:start_row + piece_height, start_col:start_col + piece_width] = piece_pattern\n",
        "\n",
        "        piece_state = np.expand_dims(piece_state, axis=0)\n",
        "        combined_state = np.concatenate((board_state, piece_state), axis=0)\n",
        "\n",
        "        return combined_state  # No need to add another batch dimension\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  # Example usage\n",
        "  env = TetrisEnv()\n",
        "  env.piece_bag = [Tetrominos.USCORE]\n",
        "  state = env.reset()\n",
        "\n",
        "  done = False\n",
        "  loop_limit = 2\n",
        "  loop = 0\n",
        "  while not done and loop < loop_limit:\n",
        "      action = env.action_space.sample()  # Random action for demonstration\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      env.board.render()\n",
        "      print(f\"Reward: {reward}, Done: {done}\")\n",
        "      print(info)\n",
        "      print(\"----------------------\")\n",
        "      loop += 1\n",
        "\n",
        "  print(env.record.__dict__)\n",
        "\n",
        "\n",
        "# main()\n"
      ],
      "metadata": {
        "id": "6yLLSvNjgz1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "#####################\n",
        "# Agent\n",
        "#\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "class TetrisCNN(nn.Module):\n",
        "    def __init__(self, input_channels, board_height, board_width, action_dim):\n",
        "        \"\"\"\n",
        "        input_channels: 1\n",
        "        board_height: 24\n",
        "        board_width: 10\n",
        "        action_dim: 40  (10 columns * 4 rotations)\n",
        "        \"\"\"\n",
        "        super(TetrisCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * board_height * board_width, 128)  # Adjust based on input size\n",
        "        self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the CNN output\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "class AgentGameInfo:\n",
        "    def __init__(self):\n",
        "        self.agent_episode = None\n",
        "        self.exploration_rate = None\n",
        "        self.batch_episode = None\n",
        "        self.batch_size = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, input_channels,\n",
        "                 board_height,\n",
        "                 board_width,\n",
        "                 action_dim,\n",
        "                 learning_rate=0.001,\n",
        "                 discount_factor=0.99,\n",
        "                 exploration_rate=1.0,\n",
        "                 exploration_decay=0.995,\n",
        "                 min_exploration_rate=0.01,\n",
        "                 replay_buffer_size=10000,\n",
        "                 batch_size=64,\n",
        "                 log_dir:str=None,\n",
        "                 load_path:str=None\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        If log_dir is not specified, no logs will be written.\n",
        "        \"\"\"\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.min_exploration_rate = min_exploration_rate\n",
        "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.reset_key = None\n",
        "        self.num_rotations = 4\n",
        "        # Show board state just before sending to model\n",
        "        self.see_model_view = False\n",
        "\n",
        "        self.board_height = board_height\n",
        "        self.board_width = board_width\n",
        "\n",
        "        self.model = TetrisCNN(input_channels, board_height, board_width, action_dim)\n",
        "        self.target_model = TetrisCNN(input_channels, board_height, board_width, action_dim)\n",
        "        self.update_target_model()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.game_records = []\n",
        "\n",
        "        self.writer = SummaryWriter(log_dir) if log_dir is not None else None\n",
        "\n",
        "\n",
        "        self.agent_episode_count = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_state(self, abspath):\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'target_model_state_dict': self.target_model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'replay_buffer': self.replay_buffer,  # You might need to serialize this if it's a custom object\n",
        "            'exploration_rate': self.exploration_rate,\n",
        "            'episode': self.agent_episode_count\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, abspath)\n",
        "\n",
        "\n",
        "\n",
        "    def load_state(self, filename=\"model.pth\"):\n",
        "        self.model.load_state_dict(torch.load(filename))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def log_game_record(self, game_record:TetrisGameRecord):\n",
        "\n",
        "        if self.writer is None:\n",
        "            print(\"WARNING: Not persisting game logs to disk\")\n",
        "\n",
        "        predict = 0\n",
        "        guess = 0\n",
        "        for was_predict in game_record.is_predict:\n",
        "            if was_predict:\n",
        "                predict += 1\n",
        "            else:\n",
        "                guess += 1\n",
        "\n",
        "        predict_rate = int(predict / (predict + guess) * 10000) / 100\n",
        "\n",
        "        r = game_record\n",
        "\n",
        "        # Wrong place to modify the record object\n",
        "        r.move_guesses = guess\n",
        "        r.move_predictions = predict\n",
        "        r.prediction_rate = predict_rate\n",
        "        r.invalid_move_pct = r.invalid_moves / (r.moves + r.invalid_moves)\n",
        "        r.avg_time_per_move = r.duration_ns / r.moves / 1000000000\n",
        "\n",
        "        print(f\"Episode {r.agent_info.batch_episode} of {r.agent_info.batch_size}. Agent run #{r.agent_info.agent_episode}\")\n",
        "        print(f\"Moves: {r.moves}\")\n",
        "        print(f\"Invalid Moves: {r.invalid_moves}\")\n",
        "        print(f\"Lines cleared: {r.lines_cleared}  ({str(r.cleared_by_size)})\")\n",
        "        print(f\"Highest Reward: {max(r.rewards)}\")\n",
        "        print(f\"Prediction Rate: {predict_rate} ({predict} of {predict+guess})\")\n",
        "        print(f\"Duration: {r.duration_ns / 1000000000}\")\n",
        "        print(f\"Agent Exploration Rate: {r.agent_info.exploration_rate}\")\n",
        "        if r.loss is not None:\n",
        "            print(f\"Loss {r.loss}\")\n",
        "\n",
        "        episode = r.agent_info.agent_episode\n",
        "\n",
        "        if not self.writer:\n",
        "            return\n",
        "\n",
        "        self.writer.add_scalar('Episode/Total Moves', r.moves, episode)\n",
        "        self.writer.add_scalar('Episode/% Invalid Moves', r.invalid_moves, episode)\n",
        "        self.writer.add_scalar('Episode/Lines Cleared', r.lines_cleared, episode)\n",
        "        self.writer.add_scalar('Episode/Cumulative Reward', r.cumulative_reward, episode)\n",
        "        self.writer.add_scalar('Episode/Prediction Rate', predict_rate, episode)\n",
        "        self.writer.add_scalar('Episode/Duration', r.duration_ns / 1000000000, episode)\n",
        "        self.writer.add_scalar('Episode/Avg Time Per Move', r.avg_time_per_move, episode)\n",
        "\n",
        "        if r.loss is not None:\n",
        "            self.writer.add_scalar('Episode/Loss', r.loss, episode)\n",
        "\n",
        "        # Used to more easily identify runs that don't\n",
        "        # have many episodes, for culling.\n",
        "        self.writer.add_scalar('Agent/Episode', episode, episode)\n",
        "\n",
        "    def save_game_records(self, filename=\"game_records.json\"):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump([record.__dict__ for record in self.game_records], f)\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def guess(self, state):\n",
        "        \"\"\"\n",
        "        Generates a random action based on the action dimensions.\n",
        "        \"\"\"\n",
        "        return (random.randint(0, self.board_width-1), random.randint(0, self.num_rotations - 1))\n",
        "\n",
        "    def predict(self, state):\n",
        "        if len(state.shape) == 3:\n",
        "            state = np.expand_dims(state, axis=0)  # Add batch dimension if not present\n",
        "        state = torch.FloatTensor(state)\n",
        "        q_values = self.model(state)\n",
        "        action_index = torch.argmax(q_values).item()\n",
        "        return (action_index // 4, action_index % 4)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.exploration_rate:\n",
        "            col = random.randint(0, self.board_width-1)\n",
        "            rotation = random.randint(0, 3)\n",
        "            return (col, rotation), False\n",
        "        else:\n",
        "            action_index = self.predict(state)\n",
        "            return action_index, True\n",
        "\n",
        "\n",
        "    def run(self, env, num_episodes=10, train=True):\n",
        "        total_rewards = []\n",
        "        target_update_interval = 10\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            self.agent_episode_count += 1\n",
        "            if env.record.moves > 0:\n",
        "                self.game_records.append(env.record)\n",
        "            state = env.reset()\n",
        "            step_count = 0\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            loss = None\n",
        "\n",
        "            while not done:\n",
        "                loss = None\n",
        "\n",
        "                if self.see_model_view:\n",
        "                    print(\"MODEL VIEW\")\n",
        "                    env.board.render()\n",
        "                    print(\"---------------------\")\n",
        "\n",
        "                if train:\n",
        "                    action, is_prediction = self.choose_action(state)\n",
        "                else:\n",
        "                    action = self.predict(state)\n",
        "                    is_prediction = True\n",
        "\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                info.is_predict = is_prediction\n",
        "                env.record.is_predict.append(is_prediction)\n",
        "                step_count += 1\n",
        "\n",
        "                if env.record.moves >= 100:\n",
        "                    print(\"Hit move cap\")\n",
        "                    done = True\n",
        "\n",
        "                if done:\n",
        "                    env.close_episode()\n",
        "\n",
        "                if train:\n",
        "                    self.remember(state, action, reward, next_state, done)\n",
        "                    loss = self.replay()\n",
        "\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            ainfo = AgentGameInfo()\n",
        "            ainfo.agent_episode = self.agent_episode_count\n",
        "            ainfo.loss = loss\n",
        "\n",
        "            # X of Y for this current execution run of the agent\n",
        "            # Within the lifecycle of this method execution.\n",
        "            ainfo.batch_episode = episode + 1 if train else episode\n",
        "            ainfo.batch_size = num_episodes\n",
        "            ainfo.exploration_rate = self.exploration_rate if train else 0\n",
        "            env.record.agent_info = ainfo\n",
        "\n",
        "            if train:\n",
        "                self.decay_exploration_rate()\n",
        "\n",
        "\n",
        "            record: TetrisGameRecord = env.record\n",
        "            record.loss = loss\n",
        "            print(f\"GAME OVER\")\n",
        "            env.render()\n",
        "            self.log_game_record(record)\n",
        "            total_rewards.append(total_reward)\n",
        "\n",
        "            if train and episode % target_update_interval == 0:\n",
        "                self.update_target_model()\n",
        "\n",
        "        return total_rewards\n",
        "\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        minibatch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        # Ensure all states have consistent shapes\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        actions = torch.LongTensor([a[0] * 4 + a[1] for a in actions])  # Ensure action is within valid range\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        max_next_q_values = self.target_model(next_states).max(1)[0]\n",
        "        expected_q_values = rewards + (self.discount_factor * max_next_q_values * (1 - dones))\n",
        "\n",
        "        loss = self.loss_fn(current_q_values, expected_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def decay_exploration_rate(self):\n",
        "        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)\n"
      ],
      "metadata": {
        "id": "-5CwQMy0yPYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change either of these values to reset the agent. Otherwise we will try\n",
        "# to keep the agent across multiple notebook cell runs.\n",
        "reset_trigger:str = str(random.randint(1,100))      # Only purpose is to reset the agent\n",
        "run_comment:str = \"CNN-cpu-all-Os-long-run-big-reward\" # @param {type:\"string\"}\n",
        "persist_logs = True # @param {type:\"boolean\"}\n",
        "\n",
        "show_board_before_running_model = True # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "has_agent = True\n",
        "try:\n",
        "    agent\n",
        "except NameError:\n",
        "    has_agent = False\n",
        "else:\n",
        "    agent:DQNAgent = agent\n",
        "    agent.see_model_view = show_board_before_running_model\n",
        "\n",
        "\n",
        "\n",
        "# Initialize Tetris environment\n",
        "env = TetrisEnv()\n",
        "env.piece_bag = [Tetrominos.O]\n",
        "\n",
        "input_channels = 1\n",
        "board_height = 24   # 20 for playfield, 4 for staging next piece\n",
        "board_width = 10\n",
        "action_dim = 40  # 4 rotations * 10 columns\n",
        "\n",
        "\n",
        "\n",
        "# This is some weird time loop stuff. Agent isn't defined until the code below\n",
        "# runs. But for future runs of this notebook cell, agent will be defined.\n",
        "reset_key = f\"{reset_trigger}-{run_comment}-\"\n",
        "if not has_agent or (has_agent and agent.reset_key != reset_key):\n",
        "    log_dir = None\n",
        "    if persist_logs:\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        log_dir = f'/content/drive/MyDrive/tensor-logs/runs/tetris-{current_time}-{run_comment}'\n",
        "    agent = DQNAgent(input_channels, board_height, board_width, action_dim, log_dir=log_dir)\n",
        "    agent.reset_key = reset_key\n",
        "    print(agent.reset_key)\n",
        "\n",
        "\n",
        "num_episodes = 10\n",
        "target_update_interval = 10\n",
        "\n",
        "training_tracker = []\n",
        "\n",
        "model_save_dir = \"/content/drive/MyDrive/tensor-logs/models\"\n",
        "\n",
        "\n",
        "def keep_training(agent):\n",
        "    \"\"\"\n",
        "    Run hueristics around recent game performance, and continue\n",
        "    training if necessary.\n",
        "    \"\"\"\n",
        "\n",
        "    if agent.agent_episode_count >= 10000:\n",
        "        return False\n",
        "\n",
        "    records = agent.game_records[-50:]\n",
        "\n",
        "    avg_line_clears_per_game = np.average([r.lines_cleared for r in records])\n",
        "    avg_moves_per_game = np.average([r.moves for r in records])\n",
        "    avg_invalid_move_pct = np.average([r.invalid_move_pct for r in records])\n",
        "\n",
        "    criteria = []\n",
        "    criteria.append(avg_line_clears_per_game > 10)\n",
        "    criteria.append(avg_moves_per_game > 80)\n",
        "    criteria.append(avg_invalid_move_pct < 1)\n",
        "\n",
        "    if np.all(criteria):\n",
        "        return False\n",
        "\n",
        "\n",
        "    # Resetting progress\n",
        "    if agent.exploration_rate < 0.1:\n",
        "        agent.exploration_rate = 0.8\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "save_interval = 500\n",
        "\n",
        "# rewards = agent.run(env, 10, train = True)\n",
        "\n",
        "while keep_training(agent):\n",
        "   rewards = agent.run(env, 50, train = True)\n",
        "\n",
        "   if agent.agent_episode_count % save_interval == 0:\n",
        "        filename = f\"tetris_ep{agent.agent_episode_count}.pth\"\n",
        "        full_path = os.path.join(model_save_dir, filename)\n",
        "        torch.save(agent.model.state_dict(), full_path)\n",
        "        print(f\"Saved model to {full_path}\")\n",
        "\n",
        "\n",
        "if keep_training(agent):\n",
        "    print(\"Training FAILED\")\n",
        "else:\n",
        "    print(\"Training SUCCEEDED!!!!!\")\n",
        "\n",
        "\n",
        "filename = f\"tetris_ep{agent.agent_episode_count}_TRAINED.pth\"\n",
        "full_path = os.path.join(model_save_dir, filename)\n",
        "torch.save(agent.model.state_dict(), full_path)\n",
        "print(f\"Saved model to {full_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent.train(env, 10)\n"
      ],
      "metadata": {
        "id": "G-4Y3mgHvlrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "records = agent.game_records\n",
        "\n",
        "for i, g in enumerate(records):\n",
        "    g.id = i\n",
        "\n",
        "    real_outcomes = len(list(filter(lambda x: (x is not None), g.placements)))\n",
        "    # print(f\"Game {i} placements {len(g.placements)}, REAL placements {real_outcomes} diff {real_outcomes-len(g.placements)}\")\n",
        "    g.real_moves = real_outcomes\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(10):\n",
        "#     print(f\"Moves: {records[i].moves}\")\n",
        "#     print(f\"Lines cleared: {records[i].lines_cleared}\")\n",
        "#     print(f\"Cumulative reward: {records[i].cumulative_reward}\")\n",
        "#     print(\"----------------------\")\n",
        "\n",
        "\n",
        "s_games = sorted(records, key=lambda x: x.real_moves, reverse=True)\n",
        "\n",
        "show_me = s_games[0]\n",
        "\n",
        "print(f\"Moves: {show_me.moves}\")\n",
        "print(f\"Lines cleared: {show_me.lines_cleared}\")\n",
        "print(f\"Boards Length: {len(show_me.boards)}\")\n",
        "print(f\"Rewards Length: {len(show_me.rewards)}\")\n",
        "\n",
        "print(show_me.placements)\n",
        "\n",
        "print(show_me.lines_cleared)\n",
        "print(show_me.cleared_by_size)\n",
        "for i in range(show_me.moves):\n",
        "    board = show_me.boards[i]\n",
        "    piece = show_me.pieces[i]\n",
        "    placement = show_me.placements[i]\n",
        "\n",
        "    if piece and placement:\n",
        "        TetrisBoard.render_state(show_me.boards[i], show_me.pieces[i]['pattern'], show_me.placements[i])\n",
        "        print(f\"Reward: {show_me.rewards[i]}\")\n",
        "        print(\"----------------------\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wtbsCEAatts4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/tensor-logs/runs\""
      ],
      "metadata": {
        "id": "dp8mo-ejTSYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}